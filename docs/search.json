[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Jenna Landy",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nGetting Started with Mutational Signatures\n\n\n\n\n\n\nMutational Signatures\n\n\nNMF\n\n\nGenomics\n\n\n\n\n\n\n\n\n\nFeb 9, 2024\n\n\nJenna Landy\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software I’ve Built",
    "section": "",
    "text": "The importance of open-source software and publicly accessible data was drilled into me while working as a software engineering intern at Project Jupyter. I’m reminded of it whenever I read an interesting paper and want to understand it better—embracing Cal Poly’s Learn by Doing motto, I want to test out the method, but this only works when the authors release code to do so.\nI hope that including open-source software with each project I work on helps my readers to understand and use my work.\n\nR Packages\n\ncausalLFO: an R package to perform causal inference for latent outcomes learned with non-negative matrix factorization with uncertainty quantification.\nbayesNMF: an R package for fitting Bayesian Non-Negative Matrix Factorization (NMF) with a variety of modeling specifications and the option to learn latent rank as part of the Bayesian model.\nfdrSAFE: an R package for estimating local (fdr) and tail-end (Fdr) false discovery rates in large-scale multiple hypothesis testing.\n\n\n\nJupyter Lab Extensions\n\njupyterlab-git: an extension to use git/GitHub within the JupyterLab environment.\neasygit: a minimal JupyterLab extension for basic version control without needing to learn git.\njupyterlab-shortcutui: a JupyterLab extension to edit keyboard shortcuts with a user interface.\nplyto: an extension to visualize training of ML models training in real time within the JupyterLab environment."
  },
  {
    "objectID": "projects/unfolding_bayesNMF/unfolding_bayesNMF.html",
    "href": "projects/unfolding_bayesNMF/unfolding_bayesNMF.html",
    "title": "Deep Unfolding Bayesian Non-Negative Matrix Factorization",
    "section": "",
    "text": "Bayesian non-negative matrix factorization is a method used across fields including genomics, audio and signal processing, and neuroscience. The complexity of the posterior of Bayesian NMF requires MCMC methods, such as a Gibbs sampler, or variational inference. We propose a faster solution through deep algorithm unrolling. By designing a neural network where each layer mimics a single iterative update, we are able to improve speed without sacrificing model performance.\n\nAdvised by Demba Ba, PhD\nCRISP Lab, Harvard School of Engineering and Applied Sciences."
  },
  {
    "objectID": "projects/causal_sigs/causal_sigs.html",
    "href": "projects/causal_sigs/causal_sigs.html",
    "title": "Causal Inference for Latent Outcomes Learned with Factor Models",
    "section": "",
    "text": "Citation: Jenna M. Landy, Dafne Zorzetto, Roberta De Vito, Giovanni Parmigiani. “Causal Inference for Latent Outcomes Learned with Factor Models.” arXiv preprint 2506.20549 (2025).\nR Software Package: causalLFO\nIn many fields—including genomics, epidemiology, natural language processing, social and behavioral sciences, and economics—it is increasingly important to address causal questions in the context of factor models or representation learning. In this work, we investigate causal effects on latent outcomes derived from high-dimensional observed data using nonnegative matrix factorization. To the best of our knowledge, this is the first study to formally address causal inference in this setting. A central challenge is that estimating a latent factor model can cause an individual’s learned latent outcome to depend on other individuals’ treatments, thereby violating the standard causal inference assumption of no interference. We formalize this issue as learning-induced interference and distinguish it from interference present in a data-generating process. To address this, we propose a novel, intuitive, and theoretically grounded algorithm to estimate causal effects on latent outcomes while mitigating learning-induced interference and improving estimation efficiency. We establish theoretical guarantees for the consistency of our estimator and demonstrate its practical utility through simulation studies and an application to cancer mutational signature analysis. All baseline and proposed methods are available in our open-source R package, causalLFO.\n\nAdvised by Giovanni Parmigiani, PhD\nDepartment of Data Science, Dana Farber Cancer Institute\nDepartment of Biostatistics, Harvard T.H. Chan School of Public Health"
  },
  {
    "objectID": "projects/success/success.html",
    "href": "projects/success/success.html",
    "title": "Studying Underlying Characteristics of Computing and Engineering Student Success (SUCCESS)",
    "section": "",
    "text": "I was a statistical consultant research assistant on the SUCCESS project, a collaboration between Cal Poly San Luis Obispo and Purdue University. In this role, I analyzed survey data on non-cognitive competencies in engineering students as predictors of academic success using R, communicated statistical results to non-statisticians, and contributed to the following three conference papers:\n\nSelf, B. P., Landy, J., Widmann, J. M., Chen, J., Kerfs, M. (2021, July), The Mechanics of SUCCESS: How Non-Cognitive and Affective Factors Relate to Academic Performance in Engineering Mechanics Paper presented at 2021 ASEE Virtual Annual Conference Content Access, Virtual Conference. https://strategy.asee.org/37876\n\nThis paper investigates how non-cognitive and affective (NCA) competencies (e.g. motivation, grit, belongingness, etc.) can better predict academic success in engineering, as measured by students grades in introductory physics, statics, and dynamics. I performed the analyses and wrote the methods, results, and discussion sections.\n\nChen, J., Landy, J. M., Scheidt, M., Major, J. C., Ge, J., Chambers, C. E., Grigorian, C., Kerfs, M., Berger, E. J., Godwin, A., Self, B. P., Widmann, J. M. (2020, June), Learning in Clusters: Exploring the Association Between Noncognitive and Affective Profiles of Engineering Students and Academic Performance Paper presented at 2020 ASEE Virtual Annual Conference Content Access, Virtual Conference. https://peer.asee.org/34901, DOI: 10.18260/1-2-34901\n\nThis paper investigates clustering students by their non-cognitive and affective (NCA) competencies and how academic success and retention differs between these groups. I performed the analyses and wrote the results section.\n\nWidmann, J., Self, B., Chen, J., Chambers, C., Kusakabe, K., Landy, J., Berger, E., Ge, J., Godwin, J., and Scheidt, M. (2019, July), Academic SUCCESS: An Analysis of How Non-Cognitive Profiles Vary by Discipline for Engineering and Computer Science Students. Paper presented at 2019 Research in Engineering Education Symposium. https://www.sasee.org.za/wp-content/uploads/REES-2019-proceedings.pdf, pages 540 - 548.\n\nThe paper looks at how non-cognative and affective competencies differ between students of different years. I administered surveys and wrote the methods section.\n\n\n\nAdvised by Jim Widmann, PhD\nMechanical Engineering Department, Cal Poly San Luis Obispo"
  },
  {
    "objectID": "projects/bayesNMF/bayesNMF.html",
    "href": "projects/bayesNMF/bayesNMF.html",
    "title": "bayesNMF: Fast Bayesian Poisson NMF with Automatically Learned Rank Applied to Mutational Signatures",
    "section": "",
    "text": "Citation: Jenna M. Landy, Nishanth Basava, and Giovanni Parmigiani. “bayesNMF: Fast Bayesian Poisson NMF with Automatically Learned Rank Applied to Mutational Signatures.” arXiv preprint arXiv:2502.18674 (2025).\nR Software Package: bayesNMF\nBayesian Non-Negative Matrix Factorization (NMF) is a method of interest in fields including genomics, neuroscience, and audio and image processing. Bayesian Poisson NMF is of particular importance for counts data, for example in cancer mutational signature analysis. However, MCMC methods for Bayesian Poisson NMF require a computationally intensive augmentation. Further, identifying latent rank is necessary, but commonly used heuristic approaches are slow and potentially subjective, while methods that learn rank automatically are unable to provide posterior uncertainties. In this paper, we introduce bayesNMF, a computationally efficient Gibbs sampler for Bayesian Poisson NMF. Metropolis-Hastings steps are used to avoid augmentation, where full conditionals from a Normal-likelihood NMF is used as geometry-informed, high-overlap proposals.\n\nWe additionally define sparse Bayesian factor inclusion (SBFI) as a method to identify rank automatically while providing posterior uncertainty quantification.\nWe provide an open-source R software package with all of the models and plotting capabilities demonstrated in this paper on GitHub at jennalandy/bayesNMF, and supplemental materials are available online.\n\nAlthough our applications focus on cancer mutational signatures, our software and results can be extended to any use of Bayesian Poisson NMF.\n\nAdvised by Giovanni Parmigiani, PhD\nDepartment of Data Science, Dana Farber Cancer Institute\nDepartment of Biostatistics, Harvard T.H. Chan School of Public Health"
  },
  {
    "objectID": "blog_posts/intro_to_signatures/intro_to_signatures.html",
    "href": "blog_posts/intro_to_signatures/intro_to_signatures.html",
    "title": "Getting Started with Mutational Signatures",
    "section": "",
    "text": "Mutational signatures model mutational processes in cancer as latent factors. This method characterizes cancer genomes in an interpretable and clinically relevant way. This article introduces the biological motivation for mutational signatures, the statistical methods used, and key literature to get you started in this field.\nReferencing this post: Landy, Jenna, “Getting Started with Mutational Signatures”, Learning the Curve, February 9, 2024."
  },
  {
    "objectID": "blog_posts/intro_to_signatures/intro_to_signatures.html#biological-motivation",
    "href": "blog_posts/intro_to_signatures/intro_to_signatures.html#biological-motivation",
    "title": "Getting Started with Mutational Signatures",
    "section": "Biological Motivation",
    "text": "Biological Motivation\nUntil recently, cancer diagnosis and treatment were based primarily on the tissue of origin, clinical presentation, and tumor morphology. The ability to sequence tumor DNA has changed this. The term “cancer” now refers to many diseases “differentiated on the basis of varying combinations of cancer gene mutations” (Van Hoeck et al. 2019).\n\n\n\n\n\nCancer is characterized by uncontrolled cell replication due to somatic mutations—mutations in non-germline cells gained through a patient’s lifetime (Pfeifer 2010). There are many well-studied biological processes that lead to somatic mutations. Some processes are exogenous in origin. For example, in non-melanoma skin cancers, sunlight exposure leaves its trace through C-to-T transitions at dipyrimidine sequences and CC-to-TT tandem mutations. Similarly, in smoking-associated lung cancers, we see G-to-T transversions, particularly at methylated CpG sites in TP53 (Pfeifer 2010). Other mutational processes are endogenous in origin (Cannataro, Mandell, and Townsend 2022). However, there are still many unknown processes that cause somatic mutations in cancer (L. B. Alexandrov et al. 2013).\n\n\n\n\n\nThough mutational signatures hadn’t yet been defined, Stratton, Cambelle, & Futreal perfectly summarized their biological motivation in 2009 (Stratton, Campbell, and Futreal 2009):\n\nthe catalogue of somatic mutations present in a cancer cell … represents a cumulative archaeological record of all the mutational processes the cancer cell has experienced throughout the lifetime of the patient. It provides a rich, and predominantly unmined, source of information for cancer epidemiologists and biologists with which to interrogate the development of individual tumors\n\nMutational signatures analysis models a tumor’s mutational landscape as the composition of multiple mutational processes working at once."
  },
  {
    "objectID": "blog_posts/intro_to_signatures/intro_to_signatures.html#defining-mutational-signatures",
    "href": "blog_posts/intro_to_signatures/intro_to_signatures.html#defining-mutational-signatures",
    "title": "Getting Started with Mutational Signatures",
    "section": "Defining Mutational Signatures",
    "text": "Defining Mutational Signatures\n\nData: Mutational Catalog \\(M\\)\nMost signatures work focuses on single base substitution (SBS) mutations, though signatures have also been developed for double base substitutions, small insertions and deletions, copy number variations, structural variation, and RNA single base substitutions (Tate et al. 2018).\nFor SBSs, a “type” of mutation is defined by the change in the sequence and the nucleotides on either side. For instance, a sequence ACT mutated to AGT has a C&gt;G substitution (the mutation) with an A on the left and a T on the right (the context). We represent this mutation type as “A[C&gt;G]T”.\nIn the simplest case, we’re not concerned with which DNA strand the mutation was on, so we only consider substitutions at a C or a T. This gives six options for the substitution (C&gt;A, C&gt;G, C&gt;T, T&gt;A, T&gt;C, T&gt;G) and four options each for the nucleotides on the left and right, so a total of \\(6 \\times 4 \\times 4\\) = 96 SBS mutation types. We use base-pair matching to convert mutations at a G or an A into one of these 96 types.\nA tumor’s mutational catalog is how often each mutation type appears across its genome. The catalog for each tumor genome \\(g\\), \\(M_{g}\\), is a vector of length 96. The full mutational catalog matrix \\(M\\) has dimension 96 x G where G is the total number of tumor genomes, and tumors’ catalogs are the columns.\n\n\\(M\\) = mutational catalog matrix, our observed data\n\\(M_{kg}\\) = count of mutation type \\(k\\) in tumor genome \\(g\\)\n\n\n\n\n\n\n\n\nLatent Factors: Mutational Signatures \\(P\\)\nA mutational signature is a mathematical representation of a mutational process defining how likely the mutational process is to give rise to each type of mutation. Mathematically, signature \\(n\\), denoted \\(P_n\\), is a probability distribution over these mutation types. The complete signatures matrix \\(P\\) has dimension 96 x N where N is the total number of signatures, and signatures are columns.\n\n\\(P\\) = signature matrix, our latent factors representing mutational processes\n\\(P_{kn}\\) = probability of mutation type \\(k\\) resulting from mutational signature \\(n\\)\n\\(P_{kn}\\) = Pr(mutation type k | signature n) such that \\(\\sum_k P_{kn} = 1 \\forall n\\)\n\nA common way to visualize individual signatures is as a bar plot of these probabilities, grouped based on the central mutation, where each group has 16 options for the left and right nucleotide context. Below is this type of plot for one signature, COSMIC SBS3.\n\n\n\n\nCOSMIC SBS3 GRCh37 (signatures v3.4). Identified in Nik-Zainal, Alexandrov, et al. (2012)."
  },
  {
    "objectID": "blog_posts/intro_to_signatures/intro_to_signatures.html#statistical-methods",
    "href": "blog_posts/intro_to_signatures/intro_to_signatures.html#statistical-methods",
    "title": "Getting Started with Mutational Signatures",
    "section": "Statistical Methods",
    "text": "Statistical Methods\nIn order to model mutational signatures, we need to convert our biological motivation about mutational processes into a mathematical expression using our mathematical definition of mutational signatures.\nBiologically, we said\n\na tumor’s mutational landscape is the composition of multiple mutational processes working at once.\n\nMathematically, we can say\n\na tumor’s mutational catalog is a linear combination of multiple mutational signatures.\n\nThe weights of this linear combination are defined by a matrix \\(E\\), where \\({E}_{ng}\\) is the number of mutations in tumor genome \\(g\\) attributed to mutational signature \\(n\\). \\(E_{ng}\\) is often called the exposure of tumor genome \\(g\\) to mutational signature \\(n\\) or, equivalently, the contribution of mutational signature \\(n\\) to tumor genome \\(g\\). This definition gives rise to the following matrix factorization problem:\n\\[\n\\begin{aligned}\nM_g =& \\sum_n P_n E_{ng}\\\\\nM = PE, &\\text{ all non-negative}.\n\\end{aligned}\n\\]\nA necessary restriction is that all our matrices of interest, \\(M\\), \\(P\\), and \\(E\\) have non-negative values. The non-negativity constraint is trivial for the first two because \\(M\\) is a counts matrix and \\(P\\) a matrix of probabilities. The choice for \\(E\\) to be non-negative comes from the biology behind signatures: mutational processes can only add mutations, not get rid of them1, so these weights must be positive as well.\nThis matrix factorization has scale non-identifiability because \\(M = PE\\) and \\(M = (P/2)(2E)\\) are equivalent solutions. For this reason, we can drop the restriction that \\(\\sum_k P_{kn} = 1 \\forall n\\) and rescale the solution after the fact.\n\n\n\n\n\nNon-negative matrix factorization (NMF) is a widely used approach for this type of problem. R and Python packages can be used to run NMF on your local computer easily. The example below shows in each language (1) generating a mutational catalog matrix \\(M\\) with \\(G\\) = 20 samples and no true signal from latent factors and (2) performing NMF and extracting final estimates \\(\\hat P\\) and \\(\\hat E\\).\n\nPythonR\n\n\n\nfrom sklearn.decomposition import NMF\nimport numpy as np\nimport pandas as pd\n\nK = 96\nG = 20\nlam = 10\nrank = 4\n\n\nM = np.random.poisson(lam = lam, size = [K, G])\n\nmodel = NMF(n_components = rank)\nP_hat = model.fit_transform(M)\nE_hat = model.components_\n\n\n\n\nlibrary(NMF)\n\nK &lt;- 96\nG &lt;- 20\nlambda &lt;- 10\nrank &lt;- 4\n\nM &lt;- matrix(\n    rpois(n = K*G, lambda = lambda), \n    nrow = K, ncol = G\n)\n\nnmf_res &lt;- NMF::nmf(M, rank = rank)\nP_hat &lt;- nmf_res@fit@W\nE_hat &lt;- nmf_res@fit@H\n\n\n\n\nA separate post with the details of this algorithm and its variations is in progress."
  },
  {
    "objectID": "blog_posts/intro_to_signatures/intro_to_signatures.html#key-literature",
    "href": "blog_posts/intro_to_signatures/intro_to_signatures.html#key-literature",
    "title": "Getting Started with Mutational Signatures",
    "section": "Key Literature",
    "text": "Key Literature\nThis is a set of papers that I found very useful to get a solid footing in this field. This is by no means a comprehensive list of all mutational signatures analysis methods or applications.\nNik-Zainal, Alexandrov, et al. (2012) was the first paper to perform mutational signatures analysis as I described above. They looked at the mutational catalogs of 21 breast cancer genomes to extract five mutational signatures. They report a signature that “is likely due to deamination of 5-methylcytosine, a relatively well-characterized mutational process”, while the rest had unknown origins. Its companion paper (Nik-Zainal, Van Loo, et al. 2012) divides somatic mutations into those shared by all cells in the sample and those that are subclonal. By classifying whether mutations were early clonal, late clonal, or subclonal in regions of copy number gains, they are able to assess the relative contributions of the five signatures at different times during each cancer’s evolution.\nLudmil B. Alexandrov et al. (2013) extends these methods into a reproducible framework. They apply this framework to a mutational catalog matrix with samples from 30 different types of cancer and reveal 21 distinct signatures. Some signatures are observed in only one or a subset of cancer types, while others are shared.\nFischer et al. (2013) further extends the NMF methods to incorporate mutational opportunity and uses a probabilistic form of NMF solved using the expectation maximization (EM) algorithm rather than gradient descent. Rosales et al. (2016) uses Bayesian NMF and introduces the idea of a differential exposure score associating exposures with clinical data to see how the activity of each signature correlates to clinical results. They also show that the exposure matrix can be used to cluster tumor samples into clinically meaningful subtypes.\nNik-Zainal and Morganella (2017) again compares mutational signatures across cancer types and indicates the need for multi-study and multi-cancer methods. Grabski, Trippa, and Parmigiani (2023) satisfies this need with a Bayesian NMF approach that can jointly analyze multiple mutational catalog matrices, such as different studies or cancer types. This method allows for signatures to be specific to one study or shared by any number of studies.\nTate et al. (2018) introduces COSMIC: the Catalogue Of Somatic Mutations In Cancer, a resource kept up to date with the most recently discovered mutational signatures, their proposed etiology, and the cancer types they impact. SBS signatures can be found here and can easily be downloaded for use in analysis. COSMIC has been updated since, most recently with analysis from L. B. Alexandrov et al. (2020).\nOther important references include Baez-Ortega and Gori (2017), which reviews various mathematical models and computational techniques for mutational signature analysis, and Kucab et al. (2019), which experimentally validates the link between certain mutagens and mutational signatures. Van Hoeck et al. (2019) describes the status of mutational signature analysis and discusses future challenges."
  },
  {
    "objectID": "blog_posts/intro_to_signatures/intro_to_signatures.html#footnotes",
    "href": "blog_posts/intro_to_signatures/intro_to_signatures.html#footnotes",
    "title": "Getting Started with Mutational Signatures",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe use positive weights for mutational signatures is different from other matrix decomposition or dictionary learning applications to genomics. For example, if we were looking at RNA-seq data, a mutational process may increase or decrease gene expression, so negative weights would be allowed.↩︎"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "My Research",
    "section": "",
    "text": "This is a rolling list of my research projects. I’m broadly interested in computational approaches to understanding molecular biology and mutational processes in cancer. My work draws from unsupervised learning, multi-study and ensemble learning, deep learning, and Bayesian modeling and computation. Many of my current projects explore various aspects of mutational signatures analysis, which models mutational processes in cancer genomes using latent factors.\nSome of these are tagged as in-progress, and this list will be updated regularly. Please reach out to me if our research interests align and you would like to chat!\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nCausal Inference for Latent Outcomes Learned with Factor Models\n\n\n\n\n\n\nmutational signatures\n\n\ncausal inference\n\n\ngenomics\n\n\n\n\n\n\n\n\n\nJun 28, 2025\n\n\nJenna Landy\n\n\n\n\n\n\n\n\n\n\n\n\nbayesNMF: Fast Bayesian Poisson NMF with Automatically Learned Rank Applied to Mutational Signatures\n\n\n\n\n\n\nbayesian computation\n\n\nmutational signatures\n\n\ngenomics\n\n\n\n\n\n\n\n\n\nFeb 25, 2025\n\n\nJenna Landy\n\n\n\n\n\n\n\n\n\n\n\n\nfdrSAFE: Selective Aggregation for Local False Discovery Rate Estimation\n\n\n\n\n\n\nmultiple hypothesis testing\n\n\ngenomics\n\n\n\n\n\n\n\n\n\nJan 23, 2024\n\n\nJenna Landy\n\n\n\n\n\n\n\n\n\n\n\n\nA Transferable GNN for Perturb-Seq\n\n\n\n\n\n\nIN PROGRESS\n\n\ndeep learning\n\n\ngenomics\n\n\n\n\n\n\n\n\n\nNov 1, 2023\n\n\nJenna Landy\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Unfolding Bayesian Non-Negative Matrix Factorization\n\n\n\n\n\n\nIN PROGRESS\n\n\ndeep learning\n\n\nmutational signatures\n\n\ngenomics\n\n\n\n\n\n\n\n\n\nMay 1, 2023\n\n\nJenna Landy\n\n\n\n\n\n\n\n\n\n\n\n\nPart of Speech-Based Data Augmentation for Neural Machine Translation\n\n\n\n\n\n\ndeep learning\n\n\nNLP\n\n\n\n\n\n\n\n\n\nMar 1, 2022\n\n\nJenna Landy\n\n\n\n\n\n\n\n\n\n\n\n\nStudying Underlying Characteristics of Computing and Engineering Student Success (SUCCESS)\n\n\n\n\n\n\n\n\n\n\n\nJun 15, 2020\n\n\nJenna Landy\n\n\n\n\n\n\n\n\n\n\n\n\nPublic GitHub Notebook Corpus Research\n\n\n\n\n\n\n\n\n\n\n\nSep 1, 2019\n\n\nJenna Landy\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/jupyter_research/jupyter_research.html",
    "href": "projects/jupyter_research/jupyter_research.html",
    "title": "Public GitHub Notebook Corpus Research",
    "section": "",
    "text": "This was my independent research project as a data science intern at Amazon Web Services (AWS) in 2019. This work is available on Project Jupyter’s GitHub.\nThe goal of this project was to collect and analyze all public Jupyter Notebooks on GitHub (nearly 5 million at the time of this analyses in Summer 2019). This analysis has helped designers, developers, and researchers in the Jupyter and AWS SageMaker community quantitatively assess how people use notebooks, with an emphasis on applications in data science, machine learning, and information visualization.\nThe results of this research complement qualitative user studies and inform challenging UX questions to focus development on real user needs. This understanding of notebook applications is crucial to user-centered design. Because many of the notebooks hosted publicly on GitHub are created as part of educational endeavours such as online and in-person courses, these insights may be particularly valuable for the Jupyter education community.\n\nAdvised by Brian Granger, co-founder of Project Jupyter and Senior Principal Technologist at AWS.\nMentored by Steve Loeppky, Senior Software Development Manager for Amazon SageMaker Notebooks."
  },
  {
    "objectID": "projects/fdrSAFE/fdrSAFE.html",
    "href": "projects/fdrSAFE/fdrSAFE.html",
    "title": "fdrSAFE: Selective Aggregation for Local False Discovery Rate Estimation",
    "section": "",
    "text": "Citation: Jenna M. Landy and Giovanni Parmigiani. “fdrSAFE: Selective Aggregation for Local False Discovery Rate Estimation.” arXiv preprint arXiv:2401.12865 (2024).\nR Software Package: fdrSAFE\nEstimating local false discovery rates (fdr) is central to large-scale multiple hypothesis testing, yet different methods often produce divergent results, and there is little guidance for selecting among them. Because ground truth hypothesis labels are unobservable, standard model selection cannot be used. We present fdrSAFE (selective aggregation for fdr estimation), a data-driven selective ensembling approach that estimates model performances on synthetic datasets designed to resemble the observed data but with known ground truth. With simulation studies and an experimental spike-in transcriptomic dataset, we show that fdrSAFE achieves robust near-optimality, performing well across diverse settings where baseline model performances vary. Along with improved fdr estimates, this framework enhances replicability by replacing arbitrary model choice with a principled, data-adaptive procedure. An open-source R software package is available on GitHub at jennalandy/fdrSAFE.\n\nAdvised by Giovanni Parmigiani, PhD\nDepartment of Data Science, Dana Farber Cancer Institute\nDepartment of Biostatistics, Harvard T.H. Chan School of Public Health"
  },
  {
    "objectID": "projects/mt_augment_pos/mt_augment_pos.html",
    "href": "projects/mt_augment_pos/mt_augment_pos.html",
    "title": "Part of Speech-Based Data Augmentation for Neural Machine Translation",
    "section": "",
    "text": "Data augmentation improves accuracy of ML models for natural language processing tasks, such as neural machine translation (NMT), by increasing the amount and variety of training data. Augmentation approaches for NLP can be applied at the token-level (e.g. contextual replacement) or at the embedding-level (e.g. soft contextual replacement or mixing two sequences by averaging their embeddings with SeqMix). While prior methods keep the semantic meaning of a sentence, a weakness is that they don’t maintain syntax. We addressed this by matching POS in word replacement and token mixing, which shows up to a 1 point increase in BLEU. Further, in prior SeqMix methods, the sequences to be mixed are chosen at random, which we address by combining more similar or different sequences. We found that mixing sequences of similar length shows up to a 0.6 point improvement in BLEU. Paper and code are publicly available.\n\nAdvised by Christopher Tanner, PhD\nInstitute for Applied Computational Science, Harvard University"
  },
  {
    "objectID": "projects/perturb_gnn/perturb_gnn.html",
    "href": "projects/perturb_gnn/perturb_gnn.html",
    "title": "A Transferable GNN for Perturb-Seq",
    "section": "",
    "text": "CRISPR can be used to knock out a particular gene, or “perturb” it, so that it is no longer functional. Single cell RNA sequencing on perturbed and control cells (perturb-seq) can be used to measure the changes in gene expressions due to the perturbation, which are usually sparse. These measurements can then give insight into the functions and/or pathways of the perturbed gene. Biologists are interested in the perturbation effects of thousands of genes and their millions of combinations, but evaluating each combination experimentally is infeasible.\nWe are working on predicting changes in gene expressions given a set of perturbations utilizing a graph of known functions from the gene ontology (GO) database. We use a graph neural network to learn perturbation embeddings, which importantly, will allow predictions for combinations of perturbations that were never tested experimentally. We are focusing on the transferability of this model across datasets and making predictions that are robust across cell types and sequencing depths."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jenna Landy",
    "section": "",
    "text": "Hello! I’m a Ph.D. student in Biostatistics at Harvard and graduated with a B.S. in Statistics and Data Science from Cal Poly SLO.\n\nI research interesting problems in genomics using unsupervised learning, multi-study and ensemble learning, deep learning, and Bayesian modeling and computation.\nI develop publicly available software for every project to encourage the reproducibility and usability of my work.\nI am passionate about the creation and accessibility of educational resources for statistics and data science.\nI have just started a blog called “Learning The Curve” where I will post tutorial-style content for methods I’m interested in.\n\nView my CV here."
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources I’ve Helped Create",
    "section": "",
    "text": "I believe that knowledge, especially knowledge that can advance a field as impactful as cancer genomics, should be as readily available and as free as open-source software. Below are tutorials and textbooks that I’ve made myself, contributed to, or edited.\n\nI edited the textbook Introduction to Data Science by Rafael Irizarry (2023)\nI was a pilot tester for early drafts of the textbook Software Design by Example with Python by Greg Wilson (2023)\nI created a tutorial on Quarto for research at the beginner level, presented at Dana Farber Cancer Institute Department of Data Science Workshop Series (2023)\nI created a tutorial on R Markdown and R package development for research at the intermediate/advanced level, presented at Harvard Biostatistics Student Seminar (2022)\nI co-authored an introductory textbook on the use of databases and APIs (2020)"
  }
]